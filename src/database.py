from typing import TYPE_CHECKING, Any, Callable, Iterable

from beartype import beartype
from sqlalchemy import Boolean, Integer
from sqlalchemy import Select as SQLSelect
from sqlalchemy import String, UniqueConstraint
from sqlalchemy import Update as SQLUpdate
from sqlalchemy import UpdateBase, create_engine
from sqlalchemy import insert as other_db_insert
from sqlalchemy.dialects import mysql, postgresql, sqlite
from sqlalchemy.exc import StatementError as SQLError
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, sessionmaker
from sqlalchemy.sql._typing import _DMLTableArgument

from globals import T, run_retries, settings
from validations import logger

if TYPE_CHECKING:
    from sqlalchemy.orm import Session as SQLSession


class DBBase(DeclarativeBase):
    """
    This class serves as a base for all tables used by the bot. It should not be referenced directly, as its only purpose is running the create_all() command at the end of this file.
    """

    pass


class DBBridge(DBBase):
    """
    An SQLAlchemy ORM class representing a database table tracking existing bridges between channels and/or threads.

    Columns
    -------
    - `id (INT)`: The id number of a bridge, has `PRIMARY KEY` and `AUTO_INCREMENT`.
    - `source (VARCHAR(32))`: The ID of the bridge's source channel or thread.
    - `target (VARCHAR(32))`: The ID of the bridge's target channel or thread.

    Constraints
    -----------
    - `unique_source_target (UNIQUE(source, target))`: A combination of source and target channel or thread IDs has to be unique.
    """

    __tablename__ = "bridges"
    __table_args__ = (
        UniqueConstraint("source", "target", name="unique_source_target"),
    )

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    source: Mapped[str] = mapped_column(String(32), nullable=False)
    target: Mapped[str] = mapped_column(String(32), nullable=False)


class DBWebhook(DBBase):
    """
    An SQLAlchemy ORM class representing a database table tracking webhooks managed by the bot in each channel.

    Columns
    -------
    - `channel (VARCHAR(32))`: The ID of the bridge's target channel or thread. Has `PRIMARY KEY`.
    - `webhook (VARCHAR(32))`: The ID of the webhook attached to the target channel which bridges messages to it.
    """

    __tablename__ = "webhooks"

    channel: Mapped[str] = mapped_column(String(32), primary_key=True)
    webhook: Mapped[str] = mapped_column(String(32))


class DBMessageMap(DBBase):
    """
    An SQLAlchemy ORM class representing a database table listing the mappings between bridged messages.

    Columns
    -------
    - `id (INT)`: The id number of a mapping, has `PRIMARY KEY` and `AUTO_INCREMENT`.
    - `source_message (VARCHAR(32))`: The ID of the message in the original channel.
    - `source_channel (VARCHAR(32))`: The ID of the channel or thread that message was sent to.
    - `target_message (VARCHAR(32))`: The ID of the message generated by the bot across a bridge.
    - `forward_header_message (VARCHAR(32))`: The ID of a message that's the header for a bridged forwarded message.
    - `target_channel (VARCHAR(32))`: The ID of the channel or thread the bridged message was bridged to.
    - `target_message_order (INT)`: If the bridged message had to be sent as multiple messages due to being too long, this stores which message in the order this one is.
    - `webhook (VARCHAR(32))`: The ID of the webhook that posted the message.
    """

    __tablename__ = "message_mappings"

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    source_message: Mapped[str] = mapped_column(String(32), nullable=False)
    source_channel: Mapped[str] = mapped_column(String(32), nullable=False)
    target_message: Mapped[str] = mapped_column(String(32), nullable=False)
    forward_header_message: Mapped[str] = mapped_column(String(32), nullable=True)
    target_channel: Mapped[str] = mapped_column(String(32), nullable=False)
    target_message_order: Mapped[int] = mapped_column(Integer(), nullable=True)
    webhook: Mapped[str] = mapped_column(String(32), nullable=True)


class DBReactionMap(DBBase):
    """
    An SQLAlchemy ORM class representing a database table listing the mappings between bridged reactions.

    Columns
    -------
    - `id (INT)`: The id number of a mapping, has `PRIMARY KEY` and `AUTO_INCREMENT`.
    - `source_emoji (VARCHAR(30))`: The ID of the emoji in the source message.
    - `source_message (VARCHAR(32))`: The ID of the message in the original channel.
    - `source_channel (VARCHAR(32))`: The ID of the channel or thread that message was sent to.
    - `target_message (VARCHAR(32))`: The ID of the message that got this reaction bridged to it.
    - `target_channel (VARCHAR(32))`: The ID of the channel or thread that message is in.
    - `target_emoji_id (VARCHAR(32))`: The ID of the emoji in the target message.
    - `target_emoji_name (VARCHAR(32))`: The name of the emoji in the target message.

    Constraints
    -----------
    - `unique_emoji_source_target (UNIQUE(source_emoji, source_message, target_message))`: A combination of source emoji, source message, and target message has to be unique.
    """

    __tablename__ = "reaction_mappings"
    __table_args__ = (
        UniqueConstraint(
            "source_emoji",
            "source_message",
            "target_message",
            name="unique_emoji_source_target",
        ),
    )

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    source_emoji: Mapped[str] = mapped_column(String(32), nullable=False)
    source_message: Mapped[str] = mapped_column(String(32), nullable=False)
    source_channel: Mapped[str] = mapped_column(String(32), nullable=False)
    target_message: Mapped[str] = mapped_column(String(32), nullable=False)
    target_channel: Mapped[str] = mapped_column(String(32), nullable=False)
    target_emoji_id: Mapped[str] = mapped_column(String(32), nullable=True)
    target_emoji_name: Mapped[str] = mapped_column(String(32), nullable=True)


class DBAutoBridgeThreadChannels(DBBase):
    """
    An SQLAlchemy ORM class representing a database table listing all channels that will automatically bridge newly-created threads.

    Columns
    -------
    - `id (INT)`: The id number of an entry, has `PRIMARY KEY` and `AUTO_INCREMENT`.
    - `channel (VARCHAR(32))`: The ID of the channel that has auto-bridge-threads enabled.
    """

    __tablename__ = "auto_bridge_thread_channels"

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    channel: Mapped[str] = mapped_column(String(32), nullable=False)


class DBEmoji(DBBase):
    """
    An SQLAlchemy ORM class representing a database table storing information about emoji.

    Columns
    -------
    - `id (VARCHAR(32))`: The emoji ID, has `PRIMARY KEY`.
    - `name (VARCHAR(32))`: The name of the emoji.
    - `server_id (VARCHAR(32))`: The ID of the server this emoji belongs to.
    - `animated (BOOL)`: Whether it's an animated emoji.
    - `image_hash (VARCHAR(32))`: A hash of this emoji's image.
    - `accessible (BOOL)`: Whether the bot has access to this emoji.
    """

    __tablename__ = "emoji"

    id: Mapped[str] = mapped_column(String(32), primary_key=True)
    name: Mapped[str] = mapped_column(String(32), nullable=True)
    server_id: Mapped[str] = mapped_column(String(32), nullable=True)
    animated: Mapped[bool] = mapped_column(Boolean, nullable=True)
    image_hash: Mapped[str] = mapped_column(String(32), nullable=True)
    accessible: Mapped[bool] = mapped_column(Boolean, nullable=False)


class DBAppWhitelist(DBBase):
    """
    An SQLAlchemy ORM class representing a database table storing IDs for applications/bots that are whitelisted for bridging messages in a given channel.

    Columns
    -------
    - `id (INT)`: The id number of an entry, has `PRIMARY KEY` and `AUTO_INCREMENT`.
    - `channel (VARCHAR(32))`: The ID of a source channel.
    - `application (VARCHAR(32))`: The ID of the application whose messages should be allowed through from that channel.

    Constraints
    -----------
    - `unique_channel_app (UNIQUE(channel, application))`: A combination of channel and application IDs has to be unique.
    """

    __tablename__ = "app_whitelist"
    __table_args__ = (
        UniqueConstraint("channel", "application", name="unique_channel_app"),
    )

    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)
    channel: Mapped[str] = mapped_column(String(32), nullable=False)
    application: Mapped[str] = mapped_column(String(32), nullable=False)


@beartype
async def sql_upsert(
    *,
    table: Any,
    indices: Iterable[str],
    ignored_cols: Iterable[str] | None = None,
    **kwargs: Any,
) -> UpdateBase:
    """Return an :class:`~sqlalchemy.UpdateBase` for inserting values into a table if a set of indices is not duplicated or updating them if it is.

    Parameters
    ----------
    table : :class:`~sqlalchemy.sql._typing._DMLTableArgument`
        The table to insert into.
    indices : Iterable[str]
        A list with the names of the indices (i.e. the columns whose uniqueness will be checked).
    ignored_cols : Iterable[str] | None, optional
        A list with the names of columns whose values should not be updated but which aren't, themselves, unique indices. Defaults to None.
    **kwargs : Any
        Named arguments for the values to insert or update. The values in `indices` must be a [proper subset](https://en.wikipedia.org/wiki/Subset) of the keys in `kwargs` (i.e. all `indices` should be in `kwargs.keys()` but there should be at least one key in `kwargs` that isn't in `indices`).

    Returns
    -------
    :class:`~sqlalchemy.UpdateBase`

    Raises
    ------
    ValueError
        `indices` is not a proper subset of `kwargs.keys()`.
    :class:`~sqlalchemy.exc.StatementError`
        SQL statement inferred from arguments was invalid or database connection failed. This error can only be raised if the database dialect is not MySQL, PostgreSQL, nor SQLite.
    """
    indices = set(indices)
    insert_values = kwargs
    insert_value_keys = set(insert_values.keys())
    if not indices < insert_value_keys:
        raise ValueError("keys is not a proper subset of kwargs.keys().")

    if ignored_cols:
        ignored_cols = set(ignored_cols)
    else:
        ignored_cols = {}
    update_values = {
        key: value
        for key, value in insert_values.items()
        if key not in indices.union(ignored_cols)
    }

    db_dialect = engine.dialect.name
    if db_dialect == "mysql":
        return (
            mysql.insert(table)
            .values(**insert_values)
            .on_duplicate_key_update(**update_values)
        )
    elif db_dialect in {"postgresql", "sqlite"}:
        insert: postgresql.Insert | sqlite.Insert
        if db_dialect == "postgresql":
            insert = postgresql.insert(table)
        else:
            insert = sqlite.insert(table)

        return insert.values(**insert_values).on_conflict_do_update(
            index_elements=indices, set_=update_values
        )
    else:
        # I'll do a manual update in this case
        session = None
        try:
            with Session() as session:
                index_values = [
                    getattr(table, idx) == insert_values[idx] for idx in indices
                ]
                upsert: UpdateBase

                def select_existing(session: "SQLSession"):
                    select_table: SQLSelect[tuple[Any]] = SQLSelect(table).where(
                        *index_values
                    )
                    return session.execute(select_table).first()

                if await sql_retry(lambda: select_existing(session)):
                    # Values with those keys do exist, so I update
                    upsert = (
                        SQLUpdate(table).where(*index_values).values(**update_values)
                    )
                else:
                    upsert = other_db_insert(table).values(**insert_values)

            return upsert
        except Exception:
            if session:
                session.rollback()
                session.close()
            raise


@beartype
async def sql_insert_ignore_duplicate(
    *,
    table: Any,
    indices: Iterable[str],
    **kwargs: Any,
) -> UpdateBase:
    """Return an :class:`~sqlalchemy.UpdateBase` for inserting values into a table if a set of indices is not duplicated.

    Parameters
    ----------
    table : :class:`~sqlalchemy.sql._typing._DMLTableArgument`
        The table to insert into.
    indices : Iterable[str]
        A list with the names of the indices (i.e. the columns whose uniqueness will be checked).
    **kwargs : Any
        Named arguments for the values to insert or update. The values in `indices` must be a [proper subset](https://en.wikipedia.org/wiki/Subset) of the keys in `kwargs` (i.e. all `indices` should be in `kwargs.keys()` but there should be at least one key in `kwargs` that isn't in `indices`).

    Returns
    -------
    :class:`~sqlalchemy.UpdateBase`

    Raises
    ------
    :class:`~sqlalchemy.exc.StatementError`
        SQL statement inferred from arguments was invalid or database connection failed. This error can only be raised if the database dialect is not MySQL, PostgreSQL, nor SQLite.
    """
    indices = set(indices)
    insert_values = kwargs

    db_dialect = engine.dialect.name
    if db_dialect == "mysql":
        random_index = indices.pop()
        return (
            mysql.insert(table)
            .values(**insert_values)
            .on_duplicate_key_update(**{random_index: getattr(table, random_index)})
        )
    elif db_dialect in {"postgresql", "sqlite"}:
        insert: postgresql.Insert | sqlite.Insert
        if db_dialect == "postgresql":
            insert = postgresql.insert(table)
        else:
            insert = sqlite.insert(table)

        return insert.values(**insert_values).on_conflict_do_nothing()
    else:
        # I'll do a manual update in this case
        session = None
        try:
            with Session() as session:
                index_values = [
                    getattr(table, idx) == insert_values[idx] for idx in indices
                ]
                insert_unknown: UpdateBase

                def select_existing(session: "SQLSession"):
                    select_table: SQLSelect[tuple[Any]] = SQLSelect(table).where(
                        *index_values
                    )
                    return session.execute(select_table).first()

                if await sql_retry(lambda: select_existing(session)):
                    # Values with those keys do exist, so I do nothing
                    random_index = indices.pop()
                    insert_unknown = SQLUpdate(table).values(
                        **{random_index: getattr(table, random_index)}
                    )
                else:
                    insert_unknown = other_db_insert(table).values(**insert_values)

            return insert_unknown
        except Exception:
            if session:
                session.rollback()
                session.close()
            raise


@beartype
async def sql_retry(
    fun: Callable[..., T],
    num_retries: int = 5,
    time_to_wait: float | int = 10,
) -> T:
    """Run an SQL function and retry it every time an :class:`~sqlalchemy.exc.StatementError` occurs up to a certain maximum number of tries. If it succeeds, return its result; otherwise, raise the error.

    Parameters
    ----------
    fun : Callable[..., T]
        The function to run.
    num_retries : int, optional
        The number of times to try the function again. If set to 0 or less, will be set to 1.
    time_to_wait : float | int, optional
        Time in seconds to wait between retries; only used if `num_retries` is greater than 1. If set to 0 or less, will set `num_retries` to 1. Defaults to 5.

    Returns
    -------
    T
    """
    return await run_retries(fun, num_retries, time_to_wait, SQLError)


# Create the engine connecting to the database
logger.info("Creating engine to connect to database...")
engine = create_engine(
    f"{settings['db_dialect']}+{settings['db_driver']}://{settings['db_user']}:{settings['db_pwd']}@{settings['db_host']}:{settings['db_port']}/{settings['db_name']}",
    pool_pre_ping=True,
    pool_recycle=3600,
)
Session = sessionmaker(engine)
logger.info("Created.")

# Create all tables represented by the above classes, if they haven't already been created
logger.info("Ensuring all necessary tables exist...")
try:
    DBBase.metadata.create_all(engine)
except Exception as e:
    logger.error("An error occurred while trying to create necessary tables: %s", e)
    raise
logger.info("All necessary tables are available.")
